from typing import List, Tuple
from langchain_classic.schema import Document
from langchain_classic.prompts import PromptTemplate
from langchain_community.llms import Tongyi
from langchain_classic.chains import LLMChain
import os
from dotenv import load_dotenv
load_dotenv()
from langchain_openai import ChatOpenAI


class Response:
    """å›ç­”ç”Ÿæˆç±»ï¼Œè´Ÿè´£è°ƒç”¨å¤§æ¨¡å‹æ ¹æ®æ£€ç´¢ç»“æœç”Ÿæˆå›ç­”"""
    
    def __init__(
        self,
        api_key: str = None,
        model_name: str = "gpt-4o-mini"
    ):
        """
        åˆå§‹åŒ–å›ç­”ç”Ÿæˆå™¨
        
        Args:
            api_key: OpenAI APIå¯†é’¥
            model_name: æ¨¡å‹åç§°
        """
        # ä¼˜å…ˆä½¿ç”¨ OPENAI_API_KEY æˆ– OPENAI_API_KEY1ï¼ˆä¸ HelloWorld.ipynb ä¸€è‡´ï¼‰
        self.api_key = api_key or os.getenv("OPENAI_API_KEY") or os.getenv("OPENAI_API_KEY1")
        self.base_url = os.getenv("OPENAI_BASE_URL")
        self.model_name = model_name
        self.llm = None
        self.chain = None
        
        if not self.api_key:
            raise ValueError("è¯·åœ¨ .env ä¸­è®¾ç½® OPENAI_API_KEY æˆ– OPENAI_API_KEY1")
    
    def initialize_llm(self):
        """åˆå§‹åŒ–å¤§è¯­è¨€æ¨¡å‹ï¼ˆOpenAI via langchain_openai.ChatOpenAIï¼‰"""
        print(f"æ­£åœ¨åˆå§‹åŒ– OpenAI æ¨¡å‹: {self.model_name}")
        
        try:
            # ç›´æ¥ä½¿ç”¨ ChatOpenAIï¼Œæ”¯æŒé€šè¿‡ .env æ³¨å…¥çš„ api_key/base_url
            self.llm = ChatOpenAI(
                model=self.model_name,
                api_key=self.api_key,
                base_url=self.base_url,
                temperature=0.7,
                max_tokens=2000
            )
            print("âœ… OpenAI å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼")
        except Exception as e:
            print(f"âŒ åˆå§‹åŒ– OpenAI æ—¶å‡ºé”™: {str(e)}")
            raise
    
    def initialize_chain(self):
        """å…¼å®¹ä¸»æµç¨‹çš„åˆå§‹åŒ–å…¥å£ï¼ˆä¸å†ä½¿ç”¨ LLMChainï¼‰"""
        if self.llm is None:
            self.initialize_llm()
        print("âœ… LLM åˆå§‹åŒ–å®Œæˆï¼ˆä½¿ç”¨ OpenAI å®¢æˆ·ç«¯ï¼‰")
        
        prompt_template = self.create_prompt_template()
        
        self.chain = LLMChain(
            llm=self.llm,
            prompt=prompt_template,
            verbose=False
        )
        
        print("âœ… LLMé“¾åˆå§‹åŒ–æˆåŠŸï¼")
    
    def create_prompt_template(self) -> PromptTemplate:
        """
        åˆ›å»ºæç¤ºè¯æ¨¡æ¿
        
        Returns:
            PromptTemplateå¯¹è±¡
        """
        template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è€ƒè¯•å’¨è¯¢åŠ©æ‰‹ï¼Œè´Ÿè´£å›ç­”ç”¨æˆ·å…³äºå„ç±»è€ƒè¯•çš„é—®é¢˜ã€‚

è¯·æ ¹æ®ä»¥ä¸‹æ£€ç´¢åˆ°çš„ç›¸å…³ä¿¡æ¯ï¼Œå‡†ç¡®ã€è¯¦ç»†åœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

æ£€ç´¢åˆ°çš„ç›¸å…³ä¿¡æ¯ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

å›ç­”è¦æ±‚ï¼š
1. å¦‚æœæ£€ç´¢åˆ°çš„ä¿¡æ¯èƒ½å¤Ÿå›ç­”é—®é¢˜ï¼Œè¯·åŸºäºè¿™äº›ä¿¡æ¯ç»™å‡ºå‡†ç¡®ã€è¯¦ç»†çš„å›ç­”
2. å¦‚æœæ£€ç´¢åˆ°çš„ä¿¡æ¯ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·è¯šå®åœ°å‘ŠçŸ¥ç”¨æˆ·ï¼Œå¹¶ç»™å‡ºå¯èƒ½çš„å»ºè®®
3. å›ç­”è¦ç»“æ„æ¸…æ™°ï¼Œæ¡ç†åˆ†æ˜
4. ä½¿ç”¨å‹å¥½ã€ä¸“ä¸šçš„è¯­æ°”
5. å¦‚æœæ¶‰åŠå…·ä½“çš„æ—¶é—´ã€æµç¨‹ã€è¦æ±‚ç­‰ï¼Œè¯·ç‰¹åˆ«æ³¨æ˜

å›ç­”ï¼š"""
        
        return PromptTemplate(
            template=template,
            input_variables=["context", "question"]
        )
    
    def initialize_chain(self):
        """å…¼å®¹ä¸»æµç¨‹çš„åˆå§‹åŒ–å…¥å£ï¼ˆä¸å†ä½¿ç”¨LLMChainï¼‰"""
        if self.llm is None:
            self.initialize_llm()
        print("âœ… LLM åˆå§‹åŒ–å®Œæˆï¼ˆä½¿ç”¨ OpenAI å®¢æˆ·ç«¯ï¼‰")
        
        prompt_template = self.create_prompt_template()
        
        self.chain = LLMChain(
            llm=self.llm,
            prompt=prompt_template,
            verbose=False
        )
        
        print("âœ… LLMé“¾åˆå§‹åŒ–æˆåŠŸï¼")
    
    def format_context(
        self, 
        retrieved_docs: List[Tuple[Document, float]]
    ) -> str:
        """
        æ ¼å¼åŒ–æ£€ç´¢åˆ°çš„æ–‡æ¡£ä¸ºä¸Šä¸‹æ–‡
        
        Args:
            retrieved_docs: æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            æ ¼å¼åŒ–çš„ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        """
        if not retrieved_docs:
            return "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"
        
        context_parts = []
        
        for i, (doc, score) in enumerate(retrieved_docs, 1):
            context_parts.append(f"[æ–‡æ¡£{i}] (ç›¸å…³åº¦: {score:.4f})")
            context_parts.append(doc.page_content)
            context_parts.append("")  # ç©ºè¡Œåˆ†éš”
        
        return "\n".join(context_parts)
    
    def generate_answer(
        self,
        question: str,
        retrieved_docs: List[Tuple[Document, float]]
    ) -> str:
        """
        ç”Ÿæˆå›ç­”
        """
        if self.llm is None:
            self.initialize_llm()
        
        # æ ¼å¼åŒ–ä¸Šä¸‹æ–‡
        context = self.format_context(retrieved_docs)
        
        print(f"\nğŸ¤– æ­£åœ¨ç”Ÿæˆå›ç­”...")
        
        try:
            # ä½¿ç”¨æç¤ºæ¨¡æ¿ç”Ÿæˆç”¨æˆ·æ¶ˆæ¯ï¼ˆä¸ notebook çš„ç”¨æ³•å¯¹é½ï¼‰
            prompt_template = self.create_prompt_template()
            user_prompt = prompt_template.format(context=context, question=question)
            
            # ä½¿ç”¨ ChatOpenAI ç›´æ¥è°ƒç”¨
            response = self.llm.invoke(user_prompt)
            
            print("âœ… å›ç­”ç”Ÿæˆå®Œæˆï¼")
            return response.content.strip()
            
        except Exception as e:
            error_msg = f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}"
            print(f"âŒ {error_msg}")
            return f"æŠ±æ­‰ï¼Œ{error_msg}"
    
    def generate_answer_with_sources(
        self,
        question: str,
        retrieved_docs: List[Tuple[Document, float]],
        show_sources: bool = True
    ) -> dict:
        """
        ç”Ÿæˆå›ç­”å¹¶é™„å¸¦æ¥æºä¿¡æ¯
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            retrieved_docs: æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨
            show_sources: æ˜¯å¦æ˜¾ç¤ºæ¥æº
            
        Returns:
            åŒ…å«å›ç­”å’Œæ¥æºçš„å­—å…¸
        """
        # ç”Ÿæˆå›ç­”
        answer = self.generate_answer(question, retrieved_docs)
        
        # æå–æ¥æºä¿¡æ¯
        sources = []
        if show_sources:
            for i, (doc, score) in enumerate(retrieved_docs, 1):
                source_info = {
                    "index": i,
                    "score": score,
                    "content": doc.page_content[:200] + "...",
                    "metadata": doc.metadata
                }
                sources.append(source_info)
        
        return {
            "question": question,
            "answer": answer,
            "sources": sources
        }
    
    def display_answer(self, result: dict):
        """
        æ˜¾ç¤ºå›ç­”ç»“æœ
        
        Args:
            result: generate_answer_with_sourcesè¿”å›çš„ç»“æœ
        """
        print("\n" + "="*60)
        print("ğŸ’¬ é—®ç­”ç»“æœ")
        print("="*60)
        
        print(f"\nâ“ é—®é¢˜: {result['question']}")
        print(f"\nâœ… å›ç­”:\n{result['answer']}")
        
        if result.get('sources'):
            print(f"\nğŸ“š å‚è€ƒæ¥æº:")
            print("â”€"*60)
            for source in result['sources']:
                print(f"\n[æ¥æº{source['index']}] (ç›¸å…³åº¦: {source['score']:.4f})")
                print(f"å†…å®¹: {source['content']}")
                if source.get('metadata'):
                    print(f"å…ƒæ•°æ®: {source['metadata']}")
        
        print("\n" + "="*60)


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    from User import User
    
    # è®¾ç½®APIå¯†é’¥ï¼ˆè¯·æ›¿æ¢ä¸ºä½ çš„å®é™…APIå¯†é’¥ï¼‰
    os.environ["DASHSCOPE_API_KEY"] = "your-api-key-here"
    
    # 1. åˆ›å»ºç”¨æˆ·æŸ¥è¯¢æ¨¡å—
    current_dir = os.path.dirname(__file__)
    vector_store_path = os.path.join(current_dir, "../vector_store")
    
    user_module = User(
        vector_store_path=vector_store_path,
        index_name="exam_qa_faiss"
    )
    
    # 2. åŠ è½½ç´¢å¼•
    user_module.load_index()
    
    # 3. åˆå§‹åŒ–é‡æ’åºï¼ˆå¯é€‰ï¼‰
    # user_module.initialize_reranker()
    
    # 4. åˆ›å»ºå›ç­”ç”Ÿæˆæ¨¡å—
    response_module = Response(model_name="qwen-turbo")
    response_module.initialize_chain()
    
    # 5. æµ‹è¯•å®Œæ•´æµç¨‹
    test_question = "å¦‚ä½•æŠ¥åé«˜è€ƒï¼Ÿ"
    
    # æ£€ç´¢ç›¸å…³æ–‡æ¡£
    retrieved_docs = user_module.query(
        test_question,
        retrieve_k=10,
        final_k=3,
        use_rerank=False
    )
    
    # ç”Ÿæˆå›ç­”
    result = response_module.generate_answer_with_sources(
        test_question,
        retrieved_docs,
        show_sources=True
    )
    
    # æ˜¾ç¤ºç»“æœ
    response_module.display_answer(result)